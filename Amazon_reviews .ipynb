{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS0OgsfhN2ot",
        "outputId": "87332b9b-518e-4a01-bdc2-82cd8e5e5cc9"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark #For google collab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POaS_jWt6pCL"
      },
      "outputs": [],
      "source": [
        "#Create %Helpful column\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col\n",
        "import pandas as pd\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Read the CSV file using pandas\n",
        "df_pandas = pd.read_csv('/content/drive/MyDrive/Portfolio/Reviews.csv')\n",
        "\n",
        "# Convert pandas DataFrame to PySpark DataFrame\n",
        "df = spark.createDataFrame(df_pandas)\n",
        "\n",
        "# Apply the transformation using withColumn\n",
        "df = df.withColumn(\n",
        "  'Helpful %',\n",
        "  when(\n",
        "    col('HelpfulnessDenominator') > 0,\n",
        "    col('HelpfulnessNumerator') / col('HelpfulnessDenominator')\n",
        "  ).otherwise(-1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K-E8Q2qpO-Ds"
      },
      "outputs": [],
      "source": [
        "#Bining % Helpful\n",
        "from pyspark.sql.functions import when, col\n",
        "bins = [-1, 0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "labels = ['Empty', '0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
        "df = df.withColumn(\n",
        "    '%upvote',\n",
        "    when(col('Helpful %').isNull(), 'Empty')\n",
        "    .when((col('Helpful %') >= bins[0]) & (col('Helpful %') < bins[1]) , labels[0])\n",
        "    .when((col('Helpful %') >= bins[1]) & (col('Helpful %') < bins[2]) , labels[1])\n",
        "    .when((col('Helpful %') >= bins[2]) & (col('Helpful %') < bins[3]) , labels[2])\n",
        "    .when((col('Helpful %') >= bins[3]) & (col('Helpful %') < bins[4]) , labels[3])\n",
        "    .when((col('Helpful %') >= bins[4]) & (col('Helpful %') < bins[5]) , labels[4])\n",
        "    .when((col('Helpful %') >= bins[5]) & (col('Helpful %') < bins[6]) , labels[5])\n",
        "    .otherwise('80-100%')\n",
        ")\n",
        "#Creat %upvote column\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "#df.groupBy(\"%upvote\").agg(count(\"*\").alias(\"count\")).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iCCWpIIvfucR"
      },
      "outputs": [],
      "source": [
        "# Drop 'Empty' in column %upvote\n",
        "df_note = df.filter(col(\"%upvote\").isNotNull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd2eYMKcMC2V"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count\n",
        "# Group by 'Score' and '%upvote' columns and count the number of 'Id' values in each group\n",
        "# Rename the aggregated column to 'Count'\n",
        "df_s = df_note.groupby(['Score', '%upvote']).agg(count('Id').alias('count_id')).orderBy(['Score', '%upvote'])\n",
        "\n",
        "#Create Heatmap to check correlation\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# group the data and pivot the Score column\n",
        "df_p = df_s.groupBy('%upvote').pivot('Score').agg(sum('count_id')).fillna(0)\n",
        "\n",
        "# convert the PySpark DataFrame to a Pandas DataFrame and sort by '%upvote'\n",
        "df_p_sorted = df_p.toPandas().sort_values('%upvote')\n",
        "\n",
        "# create the heatmap using the sorted Pandas DataFrame\n",
        "sns.heatmap(df_p_sorted.set_index('%upvote'), annot=True, cmap='YlGnBu')\n",
        "plt.title('How helpful users find among user scores (sorted by %upvote)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZkEp7NoMNst"
      },
      "outputs": [],
      "source": [
        "#Modeling\n",
        "# Check distinct Score values\n",
        "df_note.select(\"Score\").distinct().orderBy(\"Score\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "bVP25enUMOYt",
        "outputId": "4c1af748-f549-436a-ba7e-f19feb98b92a"
      },
      "outputs": [],
      "source": [
        "#Countvecterizer and Logistic Regression\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator , MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assuming 'df' is a PySpark DataFrame containing 'Score' and 'Text' columns\n",
        "\n",
        "# Filter out the rows with 'Score' equal to 3\n",
        "df2 = df_note.filter(df['Score'] != 3)\n",
        "\n",
        "# Create a dictionary to map the 'Score' values to binary labels\n",
        "y_dict = {1:0, 2:0, 4:1, 5:1}\n",
        "df2 = df2.withColumn('label', col('Score').cast('double')).na.fill(0.0, subset=['label'])\n",
        "df2 = df2.replace(y_dict, subset='label')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train, test = df2.randomSplit([0.8, 0.2], seed=2)\n",
        "\n",
        "# Perform feature extraction using CountVectorizer\n",
        "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"words\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
        "count_vectorizer = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"features\")\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer])\n",
        "pipeline_fit = pipeline.fit(train)\n",
        "train_features = pipeline_fit.transform(train).select(col('features'), col('label'))\n",
        "test_features = pipeline_fit.transform(test).select(col('features'), col('label'))\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg_fit = log_reg.fit(train_features)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "predictions = log_reg_fit.transform(test_features)\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print('Model AUC:', auc)\n",
        "\n",
        "# Assuming 'predictions' is a PySpark DataFrame containing 'label' and 'prediction' columns\n",
        "avg_prediction = predictions.select(mean('prediction')).collect()[0][0]\n",
        "print('Prediction Result:', avg_prediction)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Model Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXpyUA1fMOLE"
      },
      "outputs": [],
      "source": [
        "#TF-IDF and Logistic Regression\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator , MulticlassClassificationEvaluator\n",
        "\n",
        "# Assuming 'df' is a PySpark DataFrame containing 'Score' and 'Text' columns\n",
        "\n",
        "# Filter out the rows with 'Score' equal to 3\n",
        "df2 = df_note.filter(df['Score'] != 3)\n",
        "\n",
        "# Create a dictionary to map the 'Score' values to binary labels\n",
        "y_dict = {1:0, 2:0, 4:1, 5:1}\n",
        "df2 = df2.withColumn('label', col('Score').cast('double')).na.fill(0.0, subset=['label'])\n",
        "df2 = df2.replace(y_dict, subset='label')\n",
        "# Split the data into training and testing sets\n",
        "train, test = df2.randomSplit([0.8, 0.2], seed=2)\n",
        "\n",
        "# Perform feature extraction using TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"words\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
        "hashing_tf = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"raw_features\", numFeatures=10000)\n",
        "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"features\")\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
        "pipeline_fit = pipeline.fit(train)\n",
        "train_features = pipeline_fit.transform(train).select(col('features'), col('label'))\n",
        "test_features = pipeline_fit.transform(test).select(col('features'), col('label'))\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg_fit = log_reg.fit(train_features)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "predictions = log_reg_fit.transform(test_features)\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print('Model AUC:', auc)\n",
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Assuming 'predictions' is a PySpark DataFrame containing 'label' and 'prediction' columns\n",
        "avg_prediction = predictions.select(mean('prediction')).collect()[0][0]\n",
        "print('Prediction Result:', avg_prediction)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Model Accuracy:', accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
